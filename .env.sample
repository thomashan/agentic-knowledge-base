# .env.sample
# This file contains sample environment variables for running the application.
# Copy this file to .env and fill in the values for your environment.

# --- LLM Configuration ---
# The LLM provider to use. Supported values: 'ollama', 'openrouter'
LLM_PROVIDER="ollama"

# The specific model to use.
# For Ollama: e.g., "gemma2:2b", "llama3"
# For OpenRouter: e.g., "google/gemini-pro", "openai/gpt-4o"
LLM_MODEL="gemma2:2b"

# The base URL of the LLM API.
# This is optional and defaults to http://localhost:11434 if not set.
# For Ollama, you might not need to set this if you are running it on the default port.
# For openrouter point it do the base url of openrouter
LLM_BASE_URL=http://localhost:11434

# --- OpenRouter Specific Configuration ---
# These are only needed if LLM_PROVIDER is set to 'openrouter'.

# Your OpenRouter API key.
# You can get this from https://openrouter.ai/keys
OPENROUTER_API_KEY="your_openrouter_api_key_here"

# The referer to use for OpenRouter.
# This can be any URL, but it's good practice to set it to your app's URL.
OPENROUTER_REFERER="https://agentic-knowledge-base.com"
